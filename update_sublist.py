import os
import urllib.parse
import re

def load_url_list(file_path, convert_complex=False):
    entries = []
    if not os.path.exists(file_path):
        return entries

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if "|" not in line:
                continue
            filename, url = line.strip().split("|", 1)
            if convert_complex:
                encoded_url = urllib.parse.quote(url, safe='')
                url = (
                    "https://url.v1.mk/sub?&url="
                    f"{encoded_url}&target=clash&config="
                    "https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FSleepyHeeead"
                    "%2Fsubconverter-config%40master%2Fremote-config"
                    "%2Funiversal%2Furltest.ini&emoji=false"
                    "&append_type=true&append_info=true&scv=true"
                    "&udp=true&list=true&sort=false&fdn=true"
                    "&insert=false"
                )
            entries.append((filename, url))
    return entries

def replace_url_in_text(text, new_url):
    pattern = r'(url:\s*)([^\n]+)'
    return re.sub(pattern, rf'\1{new_url}', text, count=1)

def read_previous_urls(cache_file):
    previous = {}
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            for line in f:
                if "|" not in line:
                    continue
                name, old_url = line.strip().split("|", 1)
                previous[name] = old_url
    return previous

def write_current_urls(cache_file, entries):
    with open(cache_file, "w", encoding="utf-8") as f:
        for name, url in entries:
            f.write(f"{name}|{url}\n")

def read_previous_mtime(mtime_file):
    try:
        with open(mtime_file, "r", encoding="utf-8") as f:
            return float(f.read().strip())
    except:
        return None

def write_current_mtime(mtime_file, mtime):
    with open(mtime_file, "w", encoding="utf-8") as f:
        f.write(str(mtime))

def generate_readme(output_dir, entries):
    readme_path = os.path.join(os.getcwd(), "README.md")

    lines = [
        "# ğŸ“¦ Sublist Generator",
        "",
        "> ğŸš€ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ±Ø§Ú© Clash Ø±Ùˆ Ø§Ø² Ø±ÙˆÛŒ URLÙ‡Ø§ Ùˆ Ù‚Ø§Ù„Ø¨ Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
        "",
        "## â¬‡ï¸ Ù„ÛŒÙ†Ú© ÙØ§ÛŒÙ„â€ŒÙ‡Ø§",
        "",
    ]
    for filename, _ in entries:
        file_url = f"https://github.com/10ium/MihomoSaz/raw/main/{output_dir}/{urllib.parse.quote(filename)}"
        lines.append(f"- [ğŸ“„ {filename}]({file_url})")

    lines += [
        "",
        "## âš™ï¸ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡",
        "```bash",
        "python update_sublist.py",
        "```",
        "",
        "## ğŸ“ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡",
        "- Ù‚Ø§Ù„Ø¨: `mihomo_template.txt`",
        "- Ù„ÛŒØ³Øª Ø³Ø§Ø¯Ù‡: `Simple_URL_List.txt`",
        "- Ù„ÛŒØ³Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡: `Complex_URL_list.txt`",
        f"- Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: `{output_dir}/`",
        "",
        "## ğŸ§° Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§",
        "- Python 3.x",
        "- Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø®Ø§Ø±Ø¬ÛŒ (ÙÙ‚Ø· Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)",
        "",
        "## ğŸªª License",
        "MIT License",
    ]

    with open(readme_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def main():
    url_file_simple = "Simple_URL_List.txt"
    url_file_complex = "Complex_URL_list.txt"
    template_file = "mihomo_template.txt"
    output_dir = "Sublist"
    cache_file = ".last_urls.txt"
    mtime_file = ".last_template_mtime"

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    previous_urls = read_previous_urls(cache_file)
    previous_mtime = read_previous_mtime(mtime_file)
    current_mtime = os.path.getmtime(template_file)
    template_changed = (previous_mtime is None) or (current_mtime != previous_mtime)
    if template_changed:
        print("ğŸ›  Ù‚Ø§Ù„Ø¨ mihomo_template.txt ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡Ø› Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§")

    entries = []
    
    # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    entries += load_url_list(url_file_simple)  # Ù„ÛŒØ³Øª Ø³Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    entries += load_url_list(url_file_complex, convert_complex=True)  # Ù„ÛŒØ³Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

    new_cache_entries = []
    changes_detected = False

    for filename, new_url in entries:
        old_url = previous_urls.get(filename)
        new_cache_entries.append((filename, new_url))

        if template_changed or (new_url != old_url):
            changes_detected = True
            print(f"ğŸ›  Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ: {filename}")

            with open(template_file, "r", encoding="utf-8") as tf:
                original_text = tf.read()

            modified_text = replace_url_in_text(original_text, new_url)

            with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as outf:
                outf.write(modified_text)

    write_current_urls(cache_file, new_cache_entries)
    write_current_mtime(mtime_file, current_mtime)

    # Generate README.md
    generate_readme(output_dir, entries)
    print("ğŸ“ README.md Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")

    if not changes_detected and not template_changed:
        print("âœ… Ù‡ÛŒÚ† ØªØºÛŒÛŒØ±ÛŒ Ø¯Ø± URLâ€ŒÙ‡Ø§ ÛŒØ§ Ù‚Ø§Ù„Ø¨ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª.")

if __name__ == "__main__":
    main()
